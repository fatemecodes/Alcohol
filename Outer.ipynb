Cleaning the Dataset

# importing libraries 
import scrapy
from scrapy.crawler import CrawlerProcess
import pandas as pd
import numpy as np
# import data into d1
d1=pd.read_csv("/Users/fateme/Documents/Fall2022/Capston/Capston Project/events - northeastern 20220919.csv")
# show the data information
d1
BACKYARD_ID	HASHED_EMAIL	ANONYMOUS_ID	EVENT_TEXT	TIMESTAMP	COUNTRY_NAME	SUBDIVISION_1_NAME	DMA_NAME	CONTEXT_CAMPAIGN_SOURCE	CONTEXT_CAMPAIGN_MEDIUM
0	d66f2d8a-d43f-455f-ac54-64cfba986d17	a2f1131a857091496d418fdaa4f52cbee5030f85d84193...	99dbb8ab-39d2-41e5-98d4-16dd29ef1d26	Cart Viewed	2021-04-15T08:50:57.485Z	United States	Massachusetts	BOSTON	NaN	NaN
1	d66f2d8a-d43f-455f-ac54-64cfba986d17	a2f1131a857091496d418fdaa4f52cbee5030f85d84193...	99dbb8ab-39d2-41e5-98d4-16dd29ef1d26	Initiate Checkout	2021-04-15T08:52:21.274Z	United States	Massachusetts	BOSTON	NaN	NaN
2	6f51d1e0-b5be-4a33-8f0b-5e0baba0ce99	c452c81590139de54456f1c3b2ee2233dbb58a948f39b2...	385506c3-b31b-41bc-a03f-a4cacb87f5de	Order Viewed	2021-09-27T21:01:27.146Z	United States	NaN	NaN	NaN	NaN
3	eaad1658-ea26-4cfd-8458-c32a4fba9adf	c3d31b6f9a38fc9056874487101441587dab5cb7721424...	8bae56b4-aec6-4a54-9d3d-f6406a81a9c2	Initiate Checkout	2021-08-09T22:09:06.319Z	United States	Florida	TAMPA-ST PETERSBURG-SARASOTA	NaN	NaN
4	61f89d0f-5516-4086-8759-fc23a1a33cbc	ab5636716be2afad51403bee0747d97e57e8028766c6b0...	67a46e60-e047-472a-961a-d933ae3b73b9	Order Viewed	2021-06-16T19:43:01.352Z	United States	Massachusetts	BOSTON	NaN	NaN
...	...	...	...	...	...	...	...	...	...	...
652727	09f3ea5e-36ed-448e-b9e1-d19962bb8714	NaN	143d1288-579d-4c14-bdfe-b04b39016e86	Cart Viewed	2021-02-07T21:47:02.918Z	United States	Utah	SALT LAKE CITY	NaN	NaN
652728	09f3ea5e-36ed-448e-b9e1-d19962bb8714	NaN	143d1288-579d-4c14-bdfe-b04b39016e86	Product Viewed	2021-02-07T21:48:17.143Z	United States	Utah	SALT LAKE CITY	NaN	NaN
652729	16f7fc52-cd2f-40d1-86e9-d6e6bdbd49ac	NaN	e3f290a2-1216-4bbb-b314-e5ed479d9c23	Product Viewed	2021-06-18T23:42:32.83Z	United States	Texas	DALLAS-FORT WORTH	NaN	NaN
652730	cbe8d0a7-8a35-4092-9b94-a2658026d773	NaN	cbe8d0a7-8a35-4092-9b94-a2658026d773	Product Viewed	2021-01-25T20:18:42.645Z	United States	North Carolina	CHARLOTTE	NaN	NaN
652731	4ecb52e4-fda5-42e6-a4ec-f3e7b5f74e85	NaN	706e4498-7be6-4fbf-b34e-2532e3f4cd8f	Product Viewed	2021-04-29T16:40:43.87Z	United States	California	FRESNO-VISALIA	NaN	NaN
652732 rows × 10 columns

# find the dimention 
d1.shape 
(652732, 10)
# to get the last 10 row
d1.tail(10)
BACKYARD_ID	HASHED_EMAIL	ANONYMOUS_ID	EVENT_TEXT	TIMESTAMP	COUNTRY_NAME	SUBDIVISION_1_NAME	DMA_NAME	CONTEXT_CAMPAIGN_SOURCE	CONTEXT_CAMPAIGN_MEDIUM
652722	f65014a2-0d15-416c-b891-da9bcd092f8f	NaN	d322065e-9840-4729-a288-0753a327248f	Product Added	2021-05-23T12:58:46.888Z	United States	Virginia	WASHINGTON, DC-HAGRSTWN	NaN	NaN
652723	0edd52e3-0783-4d78-877c-7f80b8bb35d4	NaN	e29d8eef-0974-45ec-8043-99f4e42a4268	Product Viewed	2021-07-14T00:07:12.555Z	United States	Illinois	CHICAGO	NaN	NaN
652724	b76e6bb3-771f-47a1-918d-477dfd8ba72e	NaN	7e8bb424-0b16-4f61-a475-bd2bdd5a6cec	Product Viewed	2021-07-10T15:36:13.707Z	United States	Texas	HOUSTON	NaN	NaN
652725	2e92a5a2-83e8-42c3-a482-276287af5508	NaN	f6314464-8cf2-4eb8-aa4f-81ac310b13f3	Product Viewed	2021-05-15T23:16:57.78Z	United States	Texas	NaN	NaN	NaN
652726	cbb3bc4f-7d4f-48c0-8502-cd63617b3fc9	NaN	fcbda8dd-b54e-439d-97de-979925fa75ea	Product Viewed	2021-05-25T21:44:35.796Z	United States	Georgia	ATLANTA	NaN	NaN
652727	09f3ea5e-36ed-448e-b9e1-d19962bb8714	NaN	143d1288-579d-4c14-bdfe-b04b39016e86	Cart Viewed	2021-02-07T21:47:02.918Z	United States	Utah	SALT LAKE CITY	NaN	NaN
652728	09f3ea5e-36ed-448e-b9e1-d19962bb8714	NaN	143d1288-579d-4c14-bdfe-b04b39016e86	Product Viewed	2021-02-07T21:48:17.143Z	United States	Utah	SALT LAKE CITY	NaN	NaN
652729	16f7fc52-cd2f-40d1-86e9-d6e6bdbd49ac	NaN	e3f290a2-1216-4bbb-b314-e5ed479d9c23	Product Viewed	2021-06-18T23:42:32.83Z	United States	Texas	DALLAS-FORT WORTH	NaN	NaN
652730	cbe8d0a7-8a35-4092-9b94-a2658026d773	NaN	cbe8d0a7-8a35-4092-9b94-a2658026d773	Product Viewed	2021-01-25T20:18:42.645Z	United States	North Carolina	CHARLOTTE	NaN	NaN
652731	4ecb52e4-fda5-42e6-a4ec-f3e7b5f74e85	NaN	706e4498-7be6-4fbf-b34e-2532e3f4cd8f	Product Viewed	2021-04-29T16:40:43.87Z	United States	California	FRESNO-VISALIA	NaN	NaN
#measure the lengh of the data
len(d1)
652732
# Import libraries
from scipy import stats
from datetime import timedelta
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
import matplotlib.pyplot as plt
from datetime import datetime, timezone
# Import library that we creat in the previous code.
import squarify
# find the unique value in each colummns
d1['EVENT_TEXT'].value_counts()
Product Viewed                      404937
NBS Session                          60406
Lead Capture                         43230
Cart Viewed                          38491
Product Added                        31318
Order Viewed                         30550
Initiate Checkout                    14656
Inspiration Object Viewed             8251
Order Completed                       8138
Product Removed                       3273
Completed Host Application            2092
Prequalify Now Click                  1840
Out Of Stock Request                  1616
Newsletter Signup                     1412
CrossSell Added                       1081
Swatch request submitted               897
NBS Visit Request submitted            340
NBS Virtual Connection submitted       117
Call Outer                              87
Name: EVENT_TEXT, dtype: int64
#Seperare Data from TIMESTAMP column and add it to the seperate column in the data.
d1['Dates'] = pd.to_datetime(d1['TIMESTAMP']).dt.date
# show the result of data after seperate Date.
d1
BACKYARD_ID	HASHED_EMAIL	ANONYMOUS_ID	EVENT_TEXT	TIMESTAMP	COUNTRY_NAME	SUBDIVISION_1_NAME	DMA_NAME	CONTEXT_CAMPAIGN_SOURCE	CONTEXT_CAMPAIGN_MEDIUM	Dates
0	d66f2d8a-d43f-455f-ac54-64cfba986d17	a2f1131a857091496d418fdaa4f52cbee5030f85d84193...	99dbb8ab-39d2-41e5-98d4-16dd29ef1d26	Cart Viewed	2021-04-15T08:50:57.485Z	United States	Massachusetts	BOSTON	NaN	NaN	2021-04-15
1	d66f2d8a-d43f-455f-ac54-64cfba986d17	a2f1131a857091496d418fdaa4f52cbee5030f85d84193...	99dbb8ab-39d2-41e5-98d4-16dd29ef1d26	Initiate Checkout	2021-04-15T08:52:21.274Z	United States	Massachusetts	BOSTON	NaN	NaN	2021-04-15
2	6f51d1e0-b5be-4a33-8f0b-5e0baba0ce99	c452c81590139de54456f1c3b2ee2233dbb58a948f39b2...	385506c3-b31b-41bc-a03f-a4cacb87f5de	Order Viewed	2021-09-27T21:01:27.146Z	United States	NaN	NaN	NaN	NaN	2021-09-27
3	eaad1658-ea26-4cfd-8458-c32a4fba9adf	c3d31b6f9a38fc9056874487101441587dab5cb7721424...	8bae56b4-aec6-4a54-9d3d-f6406a81a9c2	Initiate Checkout	2021-08-09T22:09:06.319Z	United States	Florida	TAMPA-ST PETERSBURG-SARASOTA	NaN	NaN	2021-08-09
4	61f89d0f-5516-4086-8759-fc23a1a33cbc	ab5636716be2afad51403bee0747d97e57e8028766c6b0...	67a46e60-e047-472a-961a-d933ae3b73b9	Order Viewed	2021-06-16T19:43:01.352Z	United States	Massachusetts	BOSTON	NaN	NaN	2021-06-16
...	...	...	...	...	...	...	...	...	...	...	...
652727	09f3ea5e-36ed-448e-b9e1-d19962bb8714	NaN	143d1288-579d-4c14-bdfe-b04b39016e86	Cart Viewed	2021-02-07T21:47:02.918Z	United States	Utah	SALT LAKE CITY	NaN	NaN	2021-02-07
652728	09f3ea5e-36ed-448e-b9e1-d19962bb8714	NaN	143d1288-579d-4c14-bdfe-b04b39016e86	Product Viewed	2021-02-07T21:48:17.143Z	United States	Utah	SALT LAKE CITY	NaN	NaN	2021-02-07
652729	16f7fc52-cd2f-40d1-86e9-d6e6bdbd49ac	NaN	e3f290a2-1216-4bbb-b314-e5ed479d9c23	Product Viewed	2021-06-18T23:42:32.83Z	United States	Texas	DALLAS-FORT WORTH	NaN	NaN	2021-06-18
652730	cbe8d0a7-8a35-4092-9b94-a2658026d773	NaN	cbe8d0a7-8a35-4092-9b94-a2658026d773	Product Viewed	2021-01-25T20:18:42.645Z	United States	North Carolina	CHARLOTTE	NaN	NaN	2021-01-25
652731	4ecb52e4-fda5-42e6-a4ec-f3e7b5f74e85	NaN	706e4498-7be6-4fbf-b34e-2532e3f4cd8f	Product Viewed	2021-04-29T16:40:43.87Z	United States	California	FRESNO-VISALIA	NaN	NaN	2021-04-29
652732 rows × 11 columns

#Seperare Time from TIMESTAMP column and add it to the seperate column in the data.
d1['Time'] = pd.to_datetime(d1['TIMESTAMP']).dt.time
# show the result of data after seperate Time
d1
BACKYARD_ID	HASHED_EMAIL	ANONYMOUS_ID	EVENT_TEXT	TIMESTAMP	COUNTRY_NAME	SUBDIVISION_1_NAME	DMA_NAME	CONTEXT_CAMPAIGN_SOURCE	CONTEXT_CAMPAIGN_MEDIUM	Dates	Time
0	d66f2d8a-d43f-455f-ac54-64cfba986d17	a2f1131a857091496d418fdaa4f52cbee5030f85d84193...	99dbb8ab-39d2-41e5-98d4-16dd29ef1d26	Cart Viewed	2021-04-15T08:50:57.485Z	United States	Massachusetts	BOSTON	NaN	NaN	2021-04-15	08:50:57.485000
1	d66f2d8a-d43f-455f-ac54-64cfba986d17	a2f1131a857091496d418fdaa4f52cbee5030f85d84193...	99dbb8ab-39d2-41e5-98d4-16dd29ef1d26	Initiate Checkout	2021-04-15T08:52:21.274Z	United States	Massachusetts	BOSTON	NaN	NaN	2021-04-15	08:52:21.274000
2	6f51d1e0-b5be-4a33-8f0b-5e0baba0ce99	c452c81590139de54456f1c3b2ee2233dbb58a948f39b2...	385506c3-b31b-41bc-a03f-a4cacb87f5de	Order Viewed	2021-09-27T21:01:27.146Z	United States	NaN	NaN	NaN	NaN	2021-09-27	21:01:27.146000
3	eaad1658-ea26-4cfd-8458-c32a4fba9adf	c3d31b6f9a38fc9056874487101441587dab5cb7721424...	8bae56b4-aec6-4a54-9d3d-f6406a81a9c2	Initiate Checkout	2021-08-09T22:09:06.319Z	United States	Florida	TAMPA-ST PETERSBURG-SARASOTA	NaN	NaN	2021-08-09	22:09:06.319000
4	61f89d0f-5516-4086-8759-fc23a1a33cbc	ab5636716be2afad51403bee0747d97e57e8028766c6b0...	67a46e60-e047-472a-961a-d933ae3b73b9	Order Viewed	2021-06-16T19:43:01.352Z	United States	Massachusetts	BOSTON	NaN	NaN	2021-06-16	19:43:01.352000
...	...	...	...	...	...	...	...	...	...	...	...	...
652727	09f3ea5e-36ed-448e-b9e1-d19962bb8714	NaN	143d1288-579d-4c14-bdfe-b04b39016e86	Cart Viewed	2021-02-07T21:47:02.918Z	United States	Utah	SALT LAKE CITY	NaN	NaN	2021-02-07	21:47:02.918000
652728	09f3ea5e-36ed-448e-b9e1-d19962bb8714	NaN	143d1288-579d-4c14-bdfe-b04b39016e86	Product Viewed	2021-02-07T21:48:17.143Z	United States	Utah	SALT LAKE CITY	NaN	NaN	2021-02-07	21:48:17.143000
652729	16f7fc52-cd2f-40d1-86e9-d6e6bdbd49ac	NaN	e3f290a2-1216-4bbb-b314-e5ed479d9c23	Product Viewed	2021-06-18T23:42:32.83Z	United States	Texas	DALLAS-FORT WORTH	NaN	NaN	2021-06-18	23:42:32.830000
652730	cbe8d0a7-8a35-4092-9b94-a2658026d773	NaN	cbe8d0a7-8a35-4092-9b94-a2658026d773	Product Viewed	2021-01-25T20:18:42.645Z	United States	North Carolina	CHARLOTTE	NaN	NaN	2021-01-25	20:18:42.645000
652731	4ecb52e4-fda5-42e6-a4ec-f3e7b5f74e85	NaN	706e4498-7be6-4fbf-b34e-2532e3f4cd8f	Product Viewed	2021-04-29T16:40:43.87Z	United States	California	FRESNO-VISALIA	NaN	NaN	2021-04-29	16:40:43.870000
652732 rows × 12 columns

# Add day of the weeks base of the Date information which show number of each week during the yaer.
d1['week'] =  pd.to_datetime(d1['Dates']).dt.week
/var/folders/8n/yxw8dlkj0f3br4834kw6h3p40000gn/T/ipykernel_89115/1520852999.py:2: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead.
  d1['week'] =  pd.to_datetime(d1['Dates']).dt.week
# show the result of data after new change.
d1
BACKYARD_ID	HASHED_EMAIL	ANONYMOUS_ID	EVENT_TEXT	TIMESTAMP	COUNTRY_NAME	SUBDIVISION_1_NAME	DMA_NAME	CONTEXT_CAMPAIGN_SOURCE	CONTEXT_CAMPAIGN_MEDIUM	Dates	Time	week
0	d66f2d8a-d43f-455f-ac54-64cfba986d17	a2f1131a857091496d418fdaa4f52cbee5030f85d84193...	99dbb8ab-39d2-41e5-98d4-16dd29ef1d26	Cart Viewed	2021-04-15T08:50:57.485Z	United States	Massachusetts	BOSTON	NaN	NaN	2021-04-15	08:50:57.485000	15
1	d66f2d8a-d43f-455f-ac54-64cfba986d17	a2f1131a857091496d418fdaa4f52cbee5030f85d84193...	99dbb8ab-39d2-41e5-98d4-16dd29ef1d26	Initiate Checkout	2021-04-15T08:52:21.274Z	United States	Massachusetts	BOSTON	NaN	NaN	2021-04-15	08:52:21.274000	15
2	6f51d1e0-b5be-4a33-8f0b-5e0baba0ce99	c452c81590139de54456f1c3b2ee2233dbb58a948f39b2...	385506c3-b31b-41bc-a03f-a4cacb87f5de	Order Viewed	2021-09-27T21:01:27.146Z	United States	NaN	NaN	NaN	NaN	2021-09-27	21:01:27.146000	39
3	eaad1658-ea26-4cfd-8458-c32a4fba9adf	c3d31b6f9a38fc9056874487101441587dab5cb7721424...	8bae56b4-aec6-4a54-9d3d-f6406a81a9c2	Initiate Checkout	2021-08-09T22:09:06.319Z	United States	Florida	TAMPA-ST PETERSBURG-SARASOTA	NaN	NaN	2021-08-09	22:09:06.319000	32
4	61f89d0f-5516-4086-8759-fc23a1a33cbc	ab5636716be2afad51403bee0747d97e57e8028766c6b0...	67a46e60-e047-472a-961a-d933ae3b73b9	Order Viewed	2021-06-16T19:43:01.352Z	United States	Massachusetts	BOSTON	NaN	NaN	2021-06-16	19:43:01.352000	24
...	...	...	...	...	...	...	...	...	...	...	...	...	...
652727	09f3ea5e-36ed-448e-b9e1-d19962bb8714	NaN	143d1288-579d-4c14-bdfe-b04b39016e86	Cart Viewed	2021-02-07T21:47:02.918Z	United States	Utah	SALT LAKE CITY	NaN	NaN	2021-02-07	21:47:02.918000	5
652728	09f3ea5e-36ed-448e-b9e1-d19962bb8714	NaN	143d1288-579d-4c14-bdfe-b04b39016e86	Product Viewed	2021-02-07T21:48:17.143Z	United States	Utah	SALT LAKE CITY	NaN	NaN	2021-02-07	21:48:17.143000	5
652729	16f7fc52-cd2f-40d1-86e9-d6e6bdbd49ac	NaN	e3f290a2-1216-4bbb-b314-e5ed479d9c23	Product Viewed	2021-06-18T23:42:32.83Z	United States	Texas	DALLAS-FORT WORTH	NaN	NaN	2021-06-18	23:42:32.830000	24
652730	cbe8d0a7-8a35-4092-9b94-a2658026d773	NaN	cbe8d0a7-8a35-4092-9b94-a2658026d773	Product Viewed	2021-01-25T20:18:42.645Z	United States	North Carolina	CHARLOTTE	NaN	NaN	2021-01-25	20:18:42.645000	4
652731	4ecb52e4-fda5-42e6-a4ec-f3e7b5f74e85	NaN	706e4498-7be6-4fbf-b34e-2532e3f4cd8f	Product Viewed	2021-04-29T16:40:43.87Z	United States	California	FRESNO-VISALIA	NaN	NaN	2021-04-29	16:40:43.870000	17
652732 rows × 13 columns

#Add day of the weeks base of the Date information which dedicate number from 0 to 6 to each day 
#of week to analyise them be easier.
d1['day of week'] = pd.to_datetime(d1['Dates']).dt.weekday
#Import libraries to create the heatmap
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
plt.style.use("seaborn")
# Create a dataframe with name of all columns to prepare data to creat the heatmap.
Outer = pd.DataFrame(np.random.random((5,13)), columns=["BACKYARD_ID","HASHED_EMAIL","ANONYMOUS_ID","EVENT_TEXT","COUNTRY_NAME","SUBDIVISION_1_NAME","CONTEXT_CAMPAIGN_SOURCE","DMA_NAME","CONTEXT_CAMPAIGN_MEDIUM","Dates","Time","week","day of week"])
HeatMap

# Use our new dataframe to create a heatmap and do first analsis base of that.
# The heatmap is a visualization tool that can show the correlation between columns in ourdataset.
pd=sns.heatmap(Outer.corr(), annot=True)

Webscraping

import pandas as pd
# Import Csv file base of their name which is prices.csv.
df = pd.read_csv('prices.csv')
df
item_name	item_price	item_url
0	Brown Wicker Outdoor Sofa with Armless Chairs ...	$6,950	/products/the-five-piece/pacific-fog-gray
1	Black Wicker Outdoor Sofa with Armless Chairs ...	$6,950	/products/the-five-piece-black/pacific-fog-gray
2	Teak Outdoor Sofa with Armless Chairs - 5 Seat	$8,390	/products/teak-outdoor-sofa-with-armless-chair...
3	Charcoal Aluminum Outdoor Sofa with Armless Ch...	$6,950	/products/aluminum-outdoor-sofa-with-armless-c...
4	White Aluminum Outdoor Sofa with Armless Chair...	$6,950	/products/white-aluminum-outdoor-sofa-with-arm...
...	...	...	...
370	Charcoal Aluminum Outdoor Loveseat	$2,900	/products/aluminum-outdoor-loveseat/pacific-fo...
371	White Aluminum Outdoor Loveseat	$2,900	/products/white-aluminum-outdoor-loveseat/paci...
372	Teak Outdoor Armless Loveseat	$2,970	/products/teak-outdoor-armless-loveseat/pacifi...
373	Charcoal Aluminum Outdoor Armless Loveseat	$2,700	/products/aluminum-outdoor-armless-loveseat/pa...
374	White Aluminum Outdoor Armless Loveseat	$2,700	/products/white-aluminum-outdoor-armless-loves...
375 rows × 3 columns

df
item_name	item_price	item_url
0	Brown Wicker Outdoor Sofa with Armless Chairs ...	$6,950	/products/the-five-piece/pacific-fog-gray
1	Black Wicker Outdoor Sofa with Armless Chairs ...	$6,950	/products/the-five-piece-black/pacific-fog-gray
2	Teak Outdoor Sofa with Armless Chairs - 5 Seat	$8,390	/products/teak-outdoor-sofa-with-armless-chair...
3	Charcoal Aluminum Outdoor Sofa with Armless Ch...	$6,950	/products/aluminum-outdoor-sofa-with-armless-c...
4	White Aluminum Outdoor Sofa with Armless Chair...	$6,950	/products/white-aluminum-outdoor-sofa-with-arm...
...	...	...	...
370	Charcoal Aluminum Outdoor Loveseat	$2,900	/products/aluminum-outdoor-loveseat/pacific-fo...
371	White Aluminum Outdoor Loveseat	$2,900	/products/white-aluminum-outdoor-loveseat/paci...
372	Teak Outdoor Armless Loveseat	$2,970	/products/teak-outdoor-armless-loveseat/pacifi...
373	Charcoal Aluminum Outdoor Armless Loveseat	$2,700	/products/aluminum-outdoor-armless-loveseat/pa...
374	White Aluminum Outdoor Armless Loveseat	$2,700	/products/white-aluminum-outdoor-armless-loves...
375 rows × 3 columns

# when we create the excel file of the price. We tried to remove the duplicate base of there item_url.
nodup = df.drop_duplicates(subset=["item_url"])
nodup['item_price'] = nodup['item_price'].apply(lambda x: x[1:].replace(',',''))
nodup
/var/folders/8n/yxw8dlkj0f3br4834kw6h3p40000gn/T/ipykernel_89115/1025543214.py:3: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  nodup['item_price'] = nodup['item_price'].apply(lambda x: x[1:].replace(',',''))
item_name	item_price	item_url
0	Brown Wicker Outdoor Sofa with Armless Chairs ...	6950	/products/the-five-piece/pacific-fog-gray
1	Black Wicker Outdoor Sofa with Armless Chairs ...	6950	/products/the-five-piece-black/pacific-fog-gray
2	Teak Outdoor Sofa with Armless Chairs - 5 Seat	8390	/products/teak-outdoor-sofa-with-armless-chair...
3	Charcoal Aluminum Outdoor Sofa with Armless Ch...	6950	/products/aluminum-outdoor-sofa-with-armless-c...
4	White Aluminum Outdoor Sofa with Armless Chair...	6950	/products/white-aluminum-outdoor-sofa-with-arm...
...	...	...	...
263	Fire Pit Table with Ceramic Balls	250	/products/fire-pit-table-with-ceramic-balls
264	Splatter Guard, Stands, and Griddles	400	/products/fire-pit-table-accessories-bundle
265	Teak & Aluminum Outdoor Dining Table & 6 Direc...	1250	/products/teak-aluminum-outdoor-dining-table-6...
266	Teak & Aluminum Outdoor Expandable Dining Tabl...	2950	/products/teak-aluminum-outdoor-dining-table-8...
267	Teak & Aluminum Outdoor Expandable Dining Tabl...	3950	/products/teak-aluminum-outdoor-dining-table-1...
163 rows × 3 columns

# export the non duplicate price product to the csv file.
nodup.to_csv('no duplicate.csv')
#measure the avrage of all product price base of the nondouplicate scv file.
prices = pd.to_numeric(nodup['item_price'])
prices.mean()
3452.852760736196
Predictive Model

#Import libraries
import pandas as pd 
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import f1_score
from sklearn.model_selection import GridSearchCV
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
# Import data to start analysis
df = pd.read_csv('Book2.csv')
# show the top row of our dataset 
df.head()
S.no	BACKYARD_ID	HASHED_EMAIL	ANONYMOUS_ID	EVENT_TEXT	TIMESTAMP	COUNTRY_NAME	SUBDIVISION_1_NAME	DMA_NAME	CONTEXT_CAMPAIGN_SOURCE	...	dates	day_name	Weekday	month	year	week	time	date_fixed	hour	first time stamp
0	157946	000011e9-4028-44aa-914e-7c101848d6c8	NaN	828c7c35-a206-4f32-9f72-636837d77eff	NBS Session	31/01/21 18:02	United States	Arizona	NaN	NaN	...	31/01/21	Sunday	Weekend	1	2021	4	18:02:36	31/01/21 18:02	18	31/01/21 18:02
1	626168	0000735e-4e3f-4b5e-ad8f-13f08e8f5962	e52c0504b00d633142ed3558fee17463fbc4ab0f5098a1...	0000735e-4e3f-4b5e-ad8f-13f08e8f5962	Lead Capture	15/07/21 6:08	United States	Tennessee	CHATTANOOGA	facebook	...	15/07/21	Thursday	Weekday	7	2021	28	6:08:24	15/07/21 6:08	6	15/07/21 6:08
2	481225	0000faba-c88b-4529-8850-6cab2437b795	NaN	0000faba-c88b-4529-8850-6cab2437b795	Product Viewed	16/05/21 2:25	United States	Texas	HOUSTON	NaN	...	16/05/21	Sunday	Weekend	5	2021	19	2:25:17	16/05/21 2:25	2	16/05/21 2:25
3	274043	000361ca-6b31-48fe-beaa-c171367d1890	bdae16f6a3555010dd45971c71ef084e68facee12fe98c...	34bdcd11-221e-473c-8465-0853d2640808	Product Viewed	01/12/21 15:19	United States	Maryland	WASHINGTON, DC-HAGRSTWN	NaN	...	02/12/21	Thursday	Weekday	12	2021	48	18:15:44	02/12/21 18:15	18	01/12/21 15:19
4	274044	000361ca-6b31-48fe-beaa-c171367d1890	bdae16f6a3555010dd45971c71ef084e68facee12fe98c...	34bdcd11-221e-473c-8465-0853d2640808	Cart Viewed	01/12/21 15:19	United States	Maryland	WASHINGTON, DC-HAGRSTWN	NaN	...	02/12/21	Thursday	Weekday	12	2021	48	18:17:05	02/12/21 18:17	18	NaN
5 rows × 21 columns

#When we want to do visualization part.the name of the cities and state might be change because they had the same value.
# with this code the name of cities and state being fixed.
np.random.seed(31415) 
# Predict the percentages of each event text that might happan in the future base of the city and states.
State

# At first we want to start with state. In this part we should select the columns that we want to do prediction on that 
# which is states and event texts.
df_selected = df[ ['SUBDIVISION_1_NAME', 'EVENT_TEXT']  ]
# show data after do changes on that.
df_selected
SUBDIVISION_1_NAME	EVENT_TEXT
0	Arizona	NBS Session
1	Tennessee	Lead Capture
2	Texas	Product Viewed
3	Maryland	Product Viewed
4	Maryland	Cart Viewed
...	...	...
652727	Washington	Order Viewed
652728	Washington	Order Viewed
652729	Washington	Order Viewed
652730	Washington	Product Viewed
652731	Texas	Lead Capture
652732 rows × 2 columns

# Split the present data set to training and testing part. Use 80% of our data set to has the more accurate model and use the 
# 20% remain data to see if our model works proparely.
X_train, X_test, y_train, y_test = train_test_split(
    df_selected.drop('EVENT_TEXT',axis=1),df_selected['EVENT_TEXT'], test_size=0.2, random_state=0)
#Convert one categorical columns to multiple columns and trun it to the numeric to be able to countinue analysis. 
#In the other word use the dummy variable because we have the categorical data.
enc = OneHotEncoder(handle_unknown='ignore')
enc.fit(X_train)
enctrain = enc.transform(X_train)
enctest = enc.transform(X_test)
#Creat the randomforest model and train the mdel.

rfc = RandomForestClassifier(10,verbose=True,n_jobs=-1)
rfc.fit(enctrain, y_train)
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    0.4s remaining:    0.3s
[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.5s finished
RandomForestClassifier(n_estimators=10, n_jobs=-1, verbose=True)
# This part do the prediction process.
restest =rfc.predict(enctest)
[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.
[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.1s remaining:    0.0s
[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.1s finished
#Mesure the model accuracy.The F1 score is above avrage which 
#is not bad but it can be increase during the time by increase the data.
f1_score(y_test,restest,average='micro')
0.6188422560457153
# Find the names of unique states.
inputs = pd.DataFrame(df['SUBDIVISION_1_NAME'].unique())
# Transform again to the dummy variables because still we have the categorical data.
encinputs =enc.transform(inputs)
/Users/fateme/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names
  warnings.warn(
#make the predicting model in the encinputs that we transfer data to numeric.
probabilities = rfc.predict_proba(encinputs)
[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.
[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished
#transfer the predictive model to the table format which show the  Probabilities of Events for each row.
probabilities = pd.DataFrame(probabilities,columns=rfc.classes_)
# sort the predictive model base of the order completed which is one of the most important factore for Outer.
probs = probabilities.rename(index={i:inputs[0].iloc[i] for i in range(len(inputs))}).sort_values('Order Completed',ascending=False)
#transfer our prediction odel to the percentages.
probs=probs*100
#Show data after creating the predictive model.
probs
# export the predictive model to the csv.
probs.to_csv("data.csv")
# Try to use a for loop to find the 5 top Event Text that migh happen in the future for each states.
def top_n_partners(scores,top_n=3):
    metrics = np.array(scores.columns)
    records=[]
    for rec in scores.to_records():
        rec = list(rec)
        ID = rec[0]
        score_vals = rec[1:]
        inds = np.argsort(score_vals)
        top_metrics = metrics[inds][::-1]
        dic = {
            'top_score_%s' % (i+1):top_metrics[i]
            for i in range(top_n)
        }
        dic['ID'] = ID
        records.append(dic)
    top_n_df = pd.DataFrame(records)
    top_n_df.set_index('ID',inplace=True)
    return top_n_df
top 5 actions for each state

# export these five top score in the table format.
import numpy as np
top_n_partners(probs, 5)

#Make the visualization for the state which has the NBS seasion as their first top event text unlike the first top score event text
# in each states was product viewed.
# fit the information and define x and y.
wy = probs.loc[['Wyoming']]
wy = pd.DataFrame(wy).stack().reset_index()
wy.columns = ['hue', 'x', 'y']
sns.set(rc={'figure.figsize':(20,15)})
sns.set_theme(style='white')
# Creat the graph base of the information we define in the previous code.
ax=sns.barplot(data=wy,x='x',y='y')
plt.xticks(rotation=45, ha="right")
plt.xlabel('Event Text')
plt.ylabel('Probability')
plt.title('Wyoming')
ax.set(ylim=(0,80))
plt.show()
# Chose the three top and low State.
best = probs.iloc[:3]
worst = probs.iloc[-4:]
# Use this code to make sure the number of top States is on order.
best = best.stack().reset_index()
best.columns = ['hue', 'x', 'y']
# Use this code to make sure the number of worst States is on order.
worst = worst.stack().reset_index()
worst.columns = ['hue', 'x', 'y']
Top 3 state in ORDER COMPLETED

# make the plot for top 3 states that we find in the previous codes.These codes defined the font of texts,font of graph, show the color
#of background,show the value of x,y,title for the graph and show the angle of texts.
sns.set(rc={'figure.figsize':(20,15)})
sns.set(font_scale=2)
sns.set_style("white")
ax=sns.barplot(data=best,x='x',y='y', hue='hue')
plt.xticks(rotation=45, ha="right")
plt.xlabel('Event Text')
plt.ylabel('Probability')
plt.title('The 3 Highest States')
ax.set(ylim=(0,60))
plt.show()

Bottom 3 state in ORDER COMPLETED

#make the plot for states with the lowest oredr completed percentages.These codes defined the font of texts,font of graph, show the color
#of background,show the value of x,y,title for the graph and show the angle of texts.
sns.set(rc={'figure.figsize':(20,15)})
sns.set(font_scale=2)
sns.set_style("white")
ax=sns.barplot(data=worst,x='x',y='y', hue='hue')
plt.xticks(rotation=45, ha="right")
plt.xlabel('Event Text')
plt.ylabel('Probability')
plt.title('The 3 Lowest States')
ax.set(ylim=(0,78))
plt.show()

City

# In the next step we make the predictive model base of the cities. In this part we should select the columns that we want to do prediction on that 
# which is cities and event texts.
df_selected_city = df[ ['DMA_NAME', 'EVENT_TEXT']  ]
df_selected_city
## Split the present data set to training and testing part. Use 80% of our data set to has the more accurate model and use the 
# 20% remain data to see if our model works proparely.
X_train_city, X_test_city, y_train_city, y_test_city = train_test_split(
    df_selected_city.drop('EVENT_TEXT',axis=1),df_selected_city['EVENT_TEXT'], test_size=0.2, random_state=0)
#Import library
from sklearn.preprocessing import OneHotEncoder
#Convert one categorical columns to multiple columns and trun it to the numeric to be able to countinue analysis. 
#In the other word use the dummy variable because we have the categorical data.
enc_city = OneHotEncoder(handle_unknown='ignore')
enc_city.fit(X_train_city)
enctrain_city = enc_city.transform(X_train_city)
enctest_city = enc_city.transform(X_test_city)
#Creat the randomforest model and train the mdel.
rfc_city = RandomForestClassifier(10,verbose=True,n_jobs=-1)
rfc_city.fit(enctrain_city, y_train_city)
[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    0.6s remaining:    0.4s
[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.7s finished
RandomForestClassifier(n_estimators=10, n_jobs=-1, verbose=True)
#This part do the prediction process.
restest_city =rfc_city.predict(enctest_city)
[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.
[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.1s finished
#Mesure the model accuracy.The F1 score is above avrage which 
#is not bad but it is alos n0t good and should improve.
#it can be increase during the time by increase the data.
f1_score(y_test_city,restest_city,average='micro')
0.6190720583391423
#Find the names of unique Cities. It means if one city repetr mant time combine the info and measure it as one.
inputs_city = pd.DataFrame(df['DMA_NAME'].unique())
# Transform again to the dummy variables because still we have the categorical data.
encinputs_city =enc_city.transform(inputs_city)
/Users/fateme/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names
  warnings.warn(
#make the predicting model base of the cities.
probabilities_city = rfc_city.predict_proba(encinputs_city)
[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.
[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s
[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished
Probabilities of Events for each row

#transfer the predictive model to the table format which show the  Probabilities of Events for each row.
probabilities_city = pd.DataFrame(probabilities_city,columns=rfc_city.classes_)
# Sort the cities in table base of the higest Order completed. 
probs_city = probabilities_city.rename(index={i:inputs_city[0].iloc[i] for i in range(len(inputs_city))}).sort_values('Order Completed',ascending=False)
# change prediction model to the percentages.
probs_city= probs_city*100
# Show the result after Chabges.
probs_city

probs_city.to_csv('probs_city.csv')
# Try to use a for loop to find the 5 top Event Text that migh happen in the future for each cities.
def top_n_partners(scores,top_n=3):
    metrics = np.array(scores.columns)
    records=[]
    for rec in scores.to_records():
        rec = list(rec)
        ID = rec[0]
        score_vals = rec[1:]
        inds = np.argsort(score_vals)
        top_metrics = metrics[inds][::-1]
        dic = {
            'top_score_%s' % (i+1):top_metrics[i]
            for i in range(top_n)
        }
        dic['ID'] = ID
        records.append(dic)
    top_n_df = pd.DataFrame(records)
    top_n_df.set_index('ID',inplace=True)
    return top_n_df
Top 5 actions for each city

# export these five top score in the table format to be cleaner and nicer.
import numpy as np
topscity = top_n_partners(probs_city, 5)
topscity
top_score_1	top_score_2	top_score_3	top_score_4	top_score_5
ID					

# find the three best and worst city base of the order completed.
best_city = probs_city.iloc[:3]
worst_city = probs_city.iloc[-3:]
# Use this code to make sure the number of best cities is on order.
best_city = best_city.stack().reset_index()
best_city.columns = ['hue', 'x', 'y']
# Use this code to make sure the number of worst cities is on order.
worst_city = worst_city.stack().reset_index()
worst_city.columns = ['hue', 'x', 'y']
Top 3 city in ORDER COMPLETED

#make the plot for top cities to compare with each other. These codes defined the font of texts,font of graph, show the color
#of background,show the value of x,y,title for the graph and show the angle of texts.
sns.set(rc={'figure.figsize':(20,15)})
sns.set(font_scale=2)
sns.set_style("white")
ax=sns.barplot(data=best_city,x='x',y='y', hue='hue')
plt.xticks(rotation=45, ha="right")
plt.xlabel('Event Text')
plt.ylabel('Probability')
plt.title('The 3 Best Cities')
ax.set(ylim=(0,45))
plt.show()

Bottom 3 city in ORDER COMPLETED

#make the plot for Cities with the lowest oredr completed percentages in our predictive model.
#These codes defined the font of texts,font of graph, show the color
#of background,show the value of x,y,title for the graph and show the angle of texts.
sns.set(rc={'figure.figsize':(20,15)})
sns.set(font_scale=2)
sns.set_style("white")
ax=sns.barplot(data=worst_city,x='x',y='y', hue='hue')
plt.xticks(rotation=45, ha="right")
plt.xlabel('Event Text')
plt.ylabel('Probability')
plt.title('The 3 Lowest Cities')
ax.set(ylim=(0,72))
plt.show()
