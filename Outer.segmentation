# importing libraries 
import scrapy
from scrapy.crawler import CrawlerProcess
import pandas as pd
import numpy as np
# import data into d1
d1=pd.read_csv("/Users/fateme/Documents/Fall2022/Capston/Capston Project/events - northeastern 20220919.csv")
# show the data information
d1
# find the dimention 
d1.shape 
# to get the last 10 row
d1.tail(10)
#measure the lengh of the data
len(d1)
# Import libraries
from scipy import stats
from datetime import timedelta
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
import matplotlib.pyplot as plt
from datetime import datetime, timezone
# Import library that we creat in the previous code.
import squarify
# find the unique value in each colummns
d1['EVENT_TEXT'].value_counts()
#Seperare Data from TIMESTAMP column and add it to the seperate column in the data.
d1['Dates'] = pd.to_datetime(d1['TIMESTAMP']).dt.date
# show the result of data after seperate Date.
d1
#Seperare Time from TIMESTAMP column and add it to the seperate column in the data.
d1['Time'] = pd.to_datetime(d1['TIMESTAMP']).dt.time
# show the result of data after seperate Time
d1
# Add day of the weeks base of the Date information which show number of each week during the yaer.
d1['week'] =  pd.to_datetime(d1['Dates']).dt.week
# show the result of data after new change.
d1
#Add day of the weeks base of the Date information which dedicate number from 0 to 6 to each day 
#of week to analyise them be easier.
d1['day of week'] = pd.to_datetime(d1['Dates']).dt.weekday
#Import libraries to create the heatmap
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
plt.style.use("seaborn")
# Create a dataframe with name of all columns to prepare data to creat the heatmap.
Outer = pd.DataFrame(np.random.random((5,13)), columns=["BACKYARD_ID","HASHED_EMAIL","ANONYMOUS_ID","EVENT_TEXT","COUNTRY_NAME","SUBDIVISION_1_NAME","CONTEXT_CAMPAIGN_SOURCE","DMA_NAME","CONTEXT_CAMPAIGN_MEDIUM","Dates","Time","week","day of week"])
HeatMap

# Use our new dataframe to create a heatmap and do first analsis base of that.
# The heatmap is a visualization tool that can show the correlation between columns in ourdataset.
pd=sns.heatmap(Outer.corr(), annot=True)
Webscraping

import pandas as pd
# Import Csv file base of their name which is prices.csv.
df = pd.read_csv('prices.csv')
df
# when we create the excel file of the price. We tried to remove the duplicate base of there item_url.
nodup = df.drop_duplicates(subset=["item_url"])
nodup['item_price'] = nodup['item_price'].apply(lambda x: x[1:].replace(',',''))
nodup
# export the non duplicate price product to the csv file.
nodup.to_csv('no duplicate.csv')
#measure the avrage of all product price base of the nondouplicate scv file.
prices = pd.to_numeric(nodup['item_price'])
prices.mean()
Predictive Model

#Import libraries
import pandas as pd 
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import f1_score
from sklearn.model_selection import GridSearchCV
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import OneHotEncoder
# Import data to start analysis
df = pd.read_csv('Book2.csv')
# show the top row of our dataset 
df.head()
#When we want to do visualization part.the name of the cities and state might be change because they had the same value.
# with this code the name of cities and state being fixed.
np.random.seed(31415) 
# Predict the percentages of each event text that might happan in the future base of the city and states.
State

# At first we want to start with state. In this part we should select the columns that we want to do prediction on that 
# which is states and event texts.
df_selected = df[ ['SUBDIVISION_1_NAME', 'EVENT_TEXT']  ]
# show data after do changes on that.
df_selected
# Split the present data set to training and testing part. Use 80% of our data set to has the more accurate model and use the 
# 20% remain data to see if our model works proparely.
X_train, X_test, y_train, y_test = train_test_split(
    df_selected.drop('EVENT_TEXT',axis=1),df_selected['EVENT_TEXT'], test_size=0.2, random_state=0)
#Convert one categorical columns to multiple columns and trun it to the numeric to be able to countinue analysis. 
#In the other word use the dummy variable because we have the categorical data.
enc = OneHotEncoder(handle_unknown='ignore')
enc.fit(X_train)
enctrain = enc.transform(X_train)
enctest = enc.transform(X_test)
#Creat the randomforest model and train the mdel.

rfc = RandomForestClassifier(10,verbose=True,n_jobs=-1)
rfc.fit(enctrain, y_train)
# This part do the prediction process.
restest =rfc.predict(enctest)
#Mesure the model accuracy.The F1 score is above avrage which 
#is not bad but it can be increase during the time by increase the data.
f1_score(y_test,restest,average='micro')
# Find the names of unique states.
inputs = pd.DataFrame(df['SUBDIVISION_1_NAME'].unique())
# Transform again to the dummy variables because still we have the categorical data.
encinputs =enc.transform(inputs)
#make the predicting model in the encinputs that we transfer data to numeric.
probabilities = rfc.predict_proba(encinputs)
#transfer the predictive model to the table format which show the  Probabilities of Events for each row.
probabilities = pd.DataFrame(probabilities,columns=rfc.classes_)
# sort the predictive model base of the order completed which is one of the most important factore for Outer.
probs = probabilities.rename(index={i:inputs[0].iloc[i] for i in range(len(inputs))}).sort_values('Order Completed',ascending=False)
#transfer our prediction odel to the percentages.
probs=probs*100
#Show data after creating the predictive model.
probs
# export the predictive model to the csv.
probs.to_csv("data.csv")
# Try to use a for loop to find the 5 top Event Text that migh happen in the future for each states.
def top_n_partners(scores,top_n=3):
    metrics = np.array(scores.columns)
    records=[]
    for rec in scores.to_records():
        rec = list(rec)
        ID = rec[0]
        score_vals = rec[1:]
        inds = np.argsort(score_vals)
        top_metrics = metrics[inds][::-1]
        dic = {
            'top_score_%s' % (i+1):top_metrics[i]
            for i in range(top_n)
        }
        dic['ID'] = ID
        records.append(dic)
    top_n_df = pd.DataFrame(records)
    top_n_df.set_index('ID',inplace=True)
    return top_n_df
top 5 actions for each state

# export these five top score in the table format.
import numpy as np
top_n_partners(probs, 5)
Wyoming visualization

#Make the visualization for the state which has the NBS seasion as their first top event text unlike the first top score event text
# in each states was product viewed.
# fit the information and define x and y.
wy = probs.loc[['Wyoming']]
wy = pd.DataFrame(wy).stack().reset_index()
wy.columns = ['hue', 'x', 'y']
sns.set(rc={'figure.figsize':(20,15)})
sns.set_theme(style='white')
# Creat the graph base of the information we define in the previous code.
ax=sns.barplot(data=wy,x='x',y='y')
plt.xticks(rotation=45, ha="right")
plt.xlabel('Event Text')
plt.ylabel('Probability')
plt.title('Wyoming')
ax.set(ylim=(0,80))
plt.show()
# Chose the three top and low State.
best = probs.iloc[:3]
worst = probs.iloc[-4:]
# Use this code to make sure the number of top States is on order.
best = best.stack().reset_index()
best.columns = ['hue', 'x', 'y']
# Use this code to make sure the number of worst States is on order.
worst = worst.stack().reset_index()
worst.columns = ['hue', 'x', 'y']
Top 3 state in ORDER COMPLETED

# make the plot for top 3 states that we find in the previous codes.These codes defined the font of texts,font of graph, show the color
#of background,show the value of x,y,title for the graph and show the angle of texts.
sns.set(rc={'figure.figsize':(20,15)})
sns.set(font_scale=2)
sns.set_style("white")
ax=sns.barplot(data=best,x='x',y='y', hue='hue')
plt.xticks(rotation=45, ha="right")
plt.xlabel('Event Text')
plt.ylabel('Probability')
plt.title('The 3 Highest States')
ax.set(ylim=(0,60))
plt.show()
Bottom 3 state in ORDER COMPLETED

#make the plot for states with the lowest oredr completed percentages.These codes defined the font of texts,font of graph, show the color
#of background,show the value of x,y,title for the graph and show the angle of texts.
sns.set(rc={'figure.figsize':(20,15)})
sns.set(font_scale=2)
sns.set_style("white")
ax=sns.barplot(data=worst,x='x',y='y', hue='hue')
plt.xticks(rotation=45, ha="right")
plt.xlabel('Event Text')
plt.ylabel('Probability')
plt.title('The 3 Lowest States')
ax.set(ylim=(0,78))
plt.show()
City

# In the next step we make the predictive model base of the cities. In this part we should select the columns that we want to do prediction on that 
# which is cities and event texts.
df_selected_city = df[ ['DMA_NAME', 'EVENT_TEXT']  ]
df_selected_city
## Split the present data set to training and testing part. Use 80% of our data set to has the more accurate model and use the 
# 20% remain data to see if our model works proparely.
X_train_city, X_test_city, y_train_city, y_test_city = train_test_split(
    df_selected_city.drop('EVENT_TEXT',axis=1),df_selected_city['EVENT_TEXT'], test_size=0.2, random_state=0)
#Import library
from sklearn.preprocessing import OneHotEncoder
#Convert one categorical columns to multiple columns and trun it to the numeric to be able to countinue analysis. 
#In the other word use the dummy variable because we have the categorical data.
enc_city = OneHotEncoder(handle_unknown='ignore')
enc_city.fit(X_train_city)
enctrain_city = enc_city.transform(X_train_city)
enctest_city = enc_city.transform(X_test_city)
#Creat the randomforest model and train the mdel.
rfc_city = RandomForestClassifier(10,verbose=True,n_jobs=-1)
rfc_city.fit(enctrain_city, y_train_city)
#This part do the prediction process.
restest_city =rfc_city.predict(enctest_city)
#Mesure the model accuracy.The F1 score is above avrage which 
#is not bad but it is alos n0t good and should improve.
#it can be increase during the time by increase the data.
f1_score(y_test_city,restest_city,average='micro')
#Find the names of unique Cities. It means if one city repetr mant time combine the info and measure it as one.
inputs_city = pd.DataFrame(df['DMA_NAME'].unique())
# Transform again to the dummy variables because still we have the categorical data.
encinputs_city =enc_city.transform(inputs_city)
#make the predicting model base of the cities.
probabilities_city = rfc_city.predict_proba(encinputs_city)
Probabilities of Events for each row

#transfer the predictive model to the table format which show the  Probabilities of Events for each row.
probabilities_city = pd.DataFrame(probabilities_city,columns=rfc_city.classes_)
# Sort the cities in table base of the higest Order completed. 
probs_city = probabilities_city.rename(index={i:inputs_city[0].iloc[i] for i in range(len(inputs_city))}).sort_values('Order Completed',ascending=False)
# change prediction model to the percentages.
probs_city= probs_city*100
# Show the result after Chabges.
probs_city
probs_city.to_csv('probs_city.csv')
# Try to use a for loop to find the 5 top Event Text that migh happen in the future for each cities.
def top_n_partners(scores,top_n=3):
    metrics = np.array(scores.columns)
    records=[]
    for rec in scores.to_records():
        rec = list(rec)
        ID = rec[0]
        score_vals = rec[1:]
        inds = np.argsort(score_vals)
        top_metrics = metrics[inds][::-1]
        dic = {
            'top_score_%s' % (i+1):top_metrics[i]
            for i in range(top_n)
        }
        dic['ID'] = ID
        records.append(dic)
    top_n_df = pd.DataFrame(records)
    top_n_df.set_index('ID',inplace=True)
    return top_n_df
Top 5 actions for each city

# export these five top score in the table format to be cleaner and nicer.
import numpy as np
topscity = top_n_partners(probs_city, 5)
topscity
# find the three best and worst city base of the order completed.
best_city = probs_city.iloc[:3]
worst_city = probs_city.iloc[-3:]
# Use this code to make sure the number of best cities is on order.
best_city = best_city.stack().reset_index()
best_city.columns = ['hue', 'x', 'y']
# Use this code to make sure the number of worst cities is on order.
worst_city = worst_city.stack().reset_index()
worst_city.columns = ['hue', 'x', 'y']
Top 3 city in ORDER COMPLETED

#make the plot for top cities to compare with each other. These codes defined the font of texts,font of graph, show the color
#of background,show the value of x,y,title for the graph and show the angle of texts.
sns.set(rc={'figure.figsize':(20,15)})
sns.set(font_scale=2)
sns.set_style("white")
ax=sns.barplot(data=best_city,x='x',y='y', hue='hue')
plt.xticks(rotation=45, ha="right")
plt.xlabel('Event Text')
plt.ylabel('Probability')
plt.title('The 3 Best Cities')
ax.set(ylim=(0,45))
plt.show()
Bottom 3 city in ORDER COMPLETED

#make the plot for Cities with the lowest oredr completed percentages in our predictive model.
#These codes defined the font of texts,font of graph, show the color
#of background,show the value of x,y,title for the graph and show the angle of texts.
sns.set(rc={'figure.figsize':(20,15)})
sns.set(font_scale=2)
sns.set_style("white")
ax=sns.barplot(data=worst_city,x='x',y='y', hue='hue')
plt.xticks(rotation=45, ha="right")
plt.xlabel('Event Text')
plt.ylabel('Probability')
plt.title('The 3 Lowest Cities')
ax.set(ylim=(0,72))
plt.show()
